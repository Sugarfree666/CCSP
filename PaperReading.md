# Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware Perspective

> 题目：迈向可信的知识图谱推理：一个不确定性感知的视角
>
> 会议：AAAI 2025

## 研究问题

如何为 KG-LLM 的问答系统引入严格的不确定性量化机制，以提升其在高风险场景下的可信度和可靠性。

## 主要内容

### 理论基础

#### Conformal Prediction

给定一个用户设定的错误率容忍度 $α$，共形预测可以产生一个预测集$C(X_{test})$，并保证这个集合包含真实答案 $y$ 的概率至少是 $1-\alpha$。这个概率被称为**覆盖率**。

1. **定义非共形分数**： $s=S(x,y)$，这是一个衡量预测“不好”程度的函数。分数越高，表示预测 $y$ 与输入 $x$ 越不匹配，即不确定性越高。在本文中，这个函数通常是文本相似度的倒数或负值。分数越高，表示 $x_i$ 和 $y_i$ 的一致性越差。

2. **在校准集上计算分位数**：定义校准集是$\mathcal{D}^{cal}=\{(x_{i},y_{i})\}_{i=1}^{n}$，然后对校准集上的每一个样本$ (x_i,y_i)$，计算其非共形分数$s_{i}=S(x_{i},y_{i})$，得到的所有分数集合记为$S^{cal}$，然后根据用户定义的错误率$\alpha$，找到对应的分位数$q_{\alpha}^{S,\mathcal{D}_{cal}}$：
   $$
   q_{\alpha}^{S,\mathcal{D}_{cal}}=Quant(\{S(x,y)|(x,y)\in\mathcal{D}_{cal}\} , \frac{\lceil(n+1)(1-\alpha)\rceil}{n})
   $$
   $\frac{\lceil(n+1)(1-\alpha)\rceil}{n}$是一个概率，目的是理论上保证最终预测集的覆盖率至少是$1-\alpha$。

3. **构建预测集**：对于一个待测样本 $X_{test}$ 和所有可能的候选答案 $y$，我们计算它们的不确定性分数 $S(X_{test}, y)$。只有当候选答案 $y$ 的不确定性分数 $S(X_{test}, y)$ **小于或等于**这个阈值 $q_{\alpha}$ 时，该候选 $y$ 才会被纳入最终的预测集 $C(X_{test})$。
   $$
   C(X_{test})=\{y|y\in\mathcal{Y},S(X_{test},y)\le q_{\alpha}^{\mathcal{S},\mathcal{D}_{cal}}\}
   $$

#### Learn Then Test

在UAG这样的多步骤系统中，每一步（如检索、评估）都有自己的错误率 $(α1,α2,...)$。多步骤组合后，误差会积累，导致整体不再满足用户的风险要求。LTT提供了一种数据驱动的方法，来为每个组件寻找一组**有效的错误率配置** $λ=(α1,α2,α3)$，使得整个系统的**总体错误率**不超过 $α$。
$$
\mathbb{P}(sup_{\lambda\in\Lambda_{valid}}\mathbb{E}[L_{\lambda}|\mathcal{D}_{cal}]\le\alpha)\ge1-\delta
$$
在至少 $1-\delta$ 的置信度（概率）下，我们保证所选出的有效配置集合 $\Lambda_{valid}$ 中的所有配置 $\lambda$，它们各自产生的期望损失（即错误率 $\mathbb{E}[L_{\lambda}]$）的最大值，都不会超过用户最初设定的目标错误率 $\alpha$。

### 方法

UAG框架主要包含三个主要组件：**UQ-aware Candidate Retriever**（不确定性感知候选检索器）、**UQ-aware Candidate Evaluator**（不确定性感知候选评估器）和 **Global Error Rate Controller**（全局错误率控制器）

> 核心思想：用 conformal prediction 控制每一步的概率风险，再用 LTT 框架解决多步骤误差传播问题。

最终目标是构建一个预测集合 $C(X_{test})$，满足：
$$
\mathbb{P}(\mathbb{P}(\hat{e}\in\mathcal{Y}_{test},\forall\hat{e}\in C_{\lambda}(X_{test})|\mathcal{D}_{cal})\ge1-\alpha)\ge1-\delta
$$

#### 不确定性感知候选检索器

之前的大多数方法在KG上进行多跳图遍历和路径搜索时，为了找到潜在答案，通常采用 Top-K 候选选择这种基于启发式的方法。**缺乏理论基础**。

这个组件负责在KG上进行遍历，寻找可能的答案实体，它通过两个共形预测步骤来实现。

- **检索候选路径**：在遍历知识图谱的时候，决定下一步该走那条边。路径扩展规则为：
  $$
  \{s|s\in\mathcal{N}(v),S_{1}(Q||(||_{i=0}^{j-1}r_{i}),r_{j})<q_{\alpha_{1}}^{S_{1},\mathcal{D}_{cal}}\}
  $$
  其中 $Q$ 是问题，||表示拼接操作，$S_1$ 是文本相似度评分函数，$q_{\alpha_{1}}$ 是基于错误率 $\alpha_1$ 计算出的分位数阈值。$v$ 是当前节点, $s$ 是节点 $v$ 的一个邻居节点。

  **解释**：对于当前节点 $v$，考察它的每一个邻居 $s$。将问题 $Q$ 与路径上的所有关系拼接起来，与候选关系 $r_j$ 计算非共形分数。只要得分低于合格阈值 $q$，就会选择 $s$ 为下一跳扩展节点。

- **检索候选邻居**：判断当前访问的节点本身是否应该被加入到候选答案集合中。
  $$
  \{s|s\in\mathcal{N}(v),S_{1}(Q,||_{i=0}^{j}r_{i})<q_{\alpha_{2}}^{S_{1},\mathcal{D}_{cal}}\}
  $$
  对于当前节点 $v$，计算问题 $Q$ 与从起点到 $v$ 的完整路径的拼接文本的相似度。如果相似度分数高于阈值 $q$，就把这个节点加入候选集。

#### 不确定性感知候选评估器

现有框架在评估阶段，通常依赖LLM对检索到的信息进行直接推理，这样做会产生不可靠的输出缺乏理论保证。另一方面检索到的候选集可能过大，可能会包含很多不相关实体。

给定检索到的候选集 $\mathcal{C}$ 和推理路径 $\mathcal{P}$，以及 LLM 生成函数 $\Phi$。最终答案集定义为与 LLM 生成内容相似度满足阈值的候选：
$$
\{a\in\mathcal{C}|S_{1}(a,\Phi(\mathcal{P}))<q_{\alpha_{3}}^{S_{1},\mathcal{D}_{cal}}\}
$$

$\Phi(\mathcal{P})$：LLM基于推理路径生成一个答案，然后计算每个候选实体与答案的非共形分数。使用 $\alpha_3$ 对应计算出的分位数阈值 $q_{\alpha_3}^{S_1, \mathcal{D}_{cal}}$ 进行筛选。

#### 全局错误率控制器

UAG框架包含多个顺序执行的组件（候选检索器、候选评估器）。每个组件都有自己的错误率（$α1, α2, α3$）。即使每一步都保证错误率 $≤ \alpha$，整个系统的最终错误率也可能远大于 $\alpha$。

> 目标：提出一个方法在全局范围内控制整个系统的误差，而不是单步误差。找到一组针对每个步骤的**独立错误率参数**，使得它们组合起来后，整体系统的错误率正好被控制在 $\alpha$ 以内。

该控制器利用 **Learn Then Test (LTT)** 框架，将每个组件的错误率视为**超参数**进行校准。

- 将三个组件的错误率定义为一个超参数向量 $\lambda$，其中 $\alpha_1, \alpha_2, \alpha_3$ 分别对应路径检索、邻居检索和候选评估的错误率 ：
  $$
  \lambda = (\alpha_{1}, \alpha_{2}, \alpha_{3}) \in (0,1]^{3}
  $$
  为了寻找最佳组合，控制器定义了一个**有限搜索空间 $\Lambda$**。它是通过以步长 $h$ 对 $(0,1]$ 区间进行离散化，然后计算笛卡尔积生成的：
  $$
  \Lambda=\{(\alpha_{1},\alpha_{2},\alpha_{3})|\alpha_{1},\alpha_{2},\alpha_{3}\in\{h,2h,...,1\}\}
  $$

- 对于搜索空间中的每一个配置 $\lambda \in \Lambda$，控制器都会在校准集 $\mathcal{D}_{cal}$ 上进行假设检验，以判断该配置是否会导致系统整体错误率超标。

  - 零假设：$H_{0}^{\lambda}:\mathbb{E}[L_{\lambda}] > \alpha$，即假设该配置 $\lambda$ 的期望损失（系统错误率）**大于**用户允许的 $\alpha$。
  - 为了检验这个假设需要在校准数据集上计算一个p值。论文采用二项分布尾部界限（Binomial tail bound）来计算每个配置的 p 值 $p_{\lambda}$。

  $$
  p_{\lambda}=\mathbb{P}(Binom(n,\alpha)\le\sum_{D_{cal}}L_{\lambda})
  $$

- 直接挑选p值小的配置，很可能偶然选到一个看似有效实际无效的配置。因此使用FWER控制算法。



# Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with  Large Language Models

> 标题：图约束推理：基于大语言模型的知识图上的可信推理
>
> 会议：ICML 2025

## 研究问题

- **现有知识图谱增强方法的局限性**：

  - 检索式方法：依赖外部检索器，难以准确捕捉图结构；

  - 代理式方法：需要多轮交互，计算成本高、延迟大。

- 缺乏对未见知识图谱的泛化能力。

  针对以上问题提出了图约束推理框架GCR，该方法在多个知识图谱问答基准测试中实现了零幻觉、最先进的性能，并展示了对未见知识图谱的零样本泛化能力。

## 主要内容

GCR框架旨在通过将 KG 结构融入 LLM 的解码过程，来消除推理过程中的幻觉并确保推理的忠实性。GCR由三个核心部分构成：

### 知识图谱字典树构建

- 从问题中提及的实体（$e_q$）出发，通过广度优先搜索等图遍历算法，检索出在 $L$ 跳范围内的所有推理路径（$W_z$）。
-  然后利用将这些推理路径格式化为句子，并使用LLM的分词器进行分词得到一系列token序列。将这些token序列构建成一个前缀树。
- KG-Trie 成为一个约束条件，指导 LLM 的解码过程 。它能够以常数时间复杂度（$O(|\mathcal{W}_z|)$）高效遍历推理路径，显著降低了推理的计算成本和延迟。

### 图约束解码

普通的LLM解码，在生成每一个下一个token时，是从整个词表（Vocabulary）中所有可能的token中进行选择。而图约束解码则不同：在生成推理路径的部分，LLM只能从KG-Trie所允许的、在知识图谱中真实存在的那个极小的token子集中进行选择。
$$
P_\phi(a, w_z | q) = 
\underbrace{
    P_\phi(a | q, w_z)
}_{\text{Regular decoding}}
\underbrace{
    \prod_{i=1}^{|w_z|} P_\phi(w_{z_i} | q, w_{z_{1:i-1}}) 
    \cdot 
    C_G(w_{z_i} | w_{z_{1:i-1}})
}_{\text{Graph-constrained decoding}}
\\
C_G(w_{z_i} | w_{z_{1:i-1}}) = 
\begin{cases}
1, & \exists \text{prefix}(w_{z_{1:i}}, w_z), \ \exists w_z \in W_z, \\
0, & \text{else},
\end{cases}
$$
图约束解码过程将上面两个公式结合起来，指导 LLM 的 token 生成。

公式1的最左边表示在给定问题 $q$ 时，生成推理路径 $w_z$和 $a$ 的概率。目标是找到使这个联合概率最大的组合（$a$，$w_z$）。

等式右边第一部分是标准的LLM解码，第二部分是图约束解码。意思是对于推理路径中的每个token，LLM根据第 $1到i-1$ 个token和问题 $q$ 计算出下一个token是 $w_{z_i}$的概率。然后与后面的约束函数相乘，得出预测的token是否在KG中合法。

对于 $C_G$ 函数，它检查 $w_{z_{1:i}}$ 是否在KG-Trie中存在。如果存在 $C_G=1$ 意味着这个token被保留，否则为0，表示当前下一条应该舍去。

整个过程：

**步骤1: 生成推理路径$w_z$的token（Graph-Constrained部分）**

- LLM进入自回归解码模式，从特殊token（<PATH>） 开始生成 $w_z$。
- 对于$w_z$的每个位置$i（从1到|w_z|）$：
  - LLM先计算标准概率：$P_ϕ(w_{z_i} | q, w_{z_{1:i-1}})$，基于LLM的内部参数$ϕ$，预测下一个最可能的token。
  - 然后应用约束：乘以$C_G(w_{z_i} | w_{z_{1:i-1}})$。
    - 使用KG-Trie查询：检查 $w_{z_{1:i-1}} + w_{z_i}$ 是否是Trie中某个节点的有效前缀, 是否对应KG中某条路径的开头）。
    - 如果是（$C_G=1$），则保留该token的概率；否则（$C_G=0$），将该token的概率设为0，强制LLM从词汇表中排除它。

**步骤2: 生成假设答案a（切换到Regular Decoding）**

- 一旦$w_z$生成完成（到特殊token<PATH> ），切换回公式1的regular decoding：$P_ϕ(a|q, w_z)$。
- LLM基于 $q$ 和完整的$w_z$，自由生成假设答案 $a$。

- 整个$P_ϕ(a, w_z|q)$是联合概率：约束确保$w_z$忠实，regular部分确保a合理。

- 这里的LLM是一个轻量级大语言模型，专门用来生成推理路径和假设答案。它通过fine-tuning一个预训练的轻量级LLM（Llama-3.1-8B）来实现，目的是增强模型在图约束解码任务上的KG推理能力。使用的训练数据集$(q,w_z,a)$来自WebQSP和CWQ。
  $$
  L = \mathbb{E}_{(q, w_z, a) \sim \mathcal{D}_G} \log P_\phi(a, w_z | q) = \mathbb{E} \left[ \log \prod_{i=1}^{|a|} P_\phi(a_i | q, w_z, a_{1:i-1}) \prod_{j=1}^{|w_z|} P_\phi(w_{z_j} | q, w_{z_{1:j-1}}) \right]
  $$
  

### 图归纳

- 复杂问题的正确答案，往往可以通过多条不同的、但同样有效的推理路径来得出。具体做法是使用集束搜索算法与Graph-constrained Decoding结合起来。也就是说在解码的过程中，不是只保留一个最可能的token，而是保留一个大小为K的候选集。
- 将第一阶段生成的所有K组 (推理路径, 假设答案)，连同原始问题 q，一起输入到一个强大的、未经微调的通用LLM（如ChatGPT、GPT-4）中。
- 设计一个特定的提示词，引导该通用LLM进行归纳推理。













