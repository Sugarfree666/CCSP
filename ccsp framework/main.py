import sys
import json
import logging
import re
import os
import requests
from typing import List, Dict, Any, Set

# === å¼•å…¥è‡ªå®šä¹‰æ¨¡å— ===
# ç¡®ä¿ data_model.py, optimizer.py, wikidata_service.py åœ¨åŒä¸€ç›®å½•ä¸‹
from data_model import Constraint
from optimizer import ConstraintOptimizer
from wikidata_service import WikidataService
from openai import OpenAI, OpenAIError

# === é…ç½®æ—¥å¿— ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("CCSP-GraphEngine")


# ==============================================================================
# 0. å·¥å…·å‡½æ•°ï¼šå®ä½“é“¾æ¥ (è§£å†³ LLM å¹»è§‰çš„å…³é”®)
# ==============================================================================
def search_wikidata(label: str) -> str:
    """
    ä½¿ç”¨ Wikidata API æœç´¢å®ä½“çš„çœŸå® QIDã€‚
    """
    url = "https://www.wikidata.org/w/api.php"
    params = {
        "action": "wbsearchentities",
        "search": label,
        "language": "en",
        "format": "json",
        "limit": 1
    }
    headers = {
        "User-Agent": "CCSP-Bot/1.0 (Research Project - Educational Use)",
        "Accept": "application/json"
    }
    try:
        # æ·»åŠ  headers å‚æ•°
        response = requests.get(url, params=params, headers=headers, timeout=5)

        # å¢åŠ çŠ¶æ€ç æ£€æŸ¥
        if response.status_code != 200:
            logger.warning(f"[Entity Search] HTTP Error {response.status_code} for '{label}'")
            return None

        data = response.json()
        if data.get("search"):
            return data["search"][0]["id"]

    except json.JSONDecodeError:
        logger.warning(f"[Entity Search] JSON Decode Error for '{label}'. Response text: {response.text[:100]}...")
    except Exception as e:
        logger.warning(f"[Entity Search] Failed for '{label}': {e}")

    return None


# ==============================================================================
# 1. LLM æœåŠ¡ (æ”¯æŒä»£ç†ä¸æ¸…æ´—)
# ==============================================================================
class LLMService:
    def __init__(self, api_key: str, base_url: str, model: str):
        self.model = model
        self.client = OpenAI(api_key=api_key, base_url=base_url)

    def generate_text(self, prompt: str) -> str:
        """ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤ (é JSON)"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,  # ç¨å¾®æé«˜æ¸©åº¦ï¼Œè®©å›ç­”æ›´è‡ªç„¶
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"LLM Text Gen Error: {e}")
            return "Sorry, I could not generate a final answer due to an error."

    def generate_json(self, prompt: str) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆ JSON ç”Ÿæˆï¼šè‡ªåŠ¨æ¸…æ´—ç‰¹æ®Šå­—ç¬¦"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
            )
            text = response.choices[0].message.content

            # æ¸…æ´—ï¼šç§»é™¤ Markdown æ ‡è®°å’Œä¸å¯è§ç©ºæ ¼
            text = text.replace('\u00A0', ' ')
            json_match = re.search(r'\{.*\}|\[.*\]', text, re.DOTALL)

            if json_match:
                return json.loads(json_match.group(0))
            return json.loads(text)
        except Exception as e:
            logger.error(f"LLM Error: {e}")
            return {}


# ==============================================================================
# 2. Parsing (è§£æé˜¶æ®µ)
# ==============================================================================
def parse_query_to_constraints(user_query: str, llm: LLMService) -> List[Constraint]:
    logger.info("Phase 1: Parsing natural language to constraints...")

    # Prompt æ˜ç¡®è¦æ±‚ä¸çŒœ IDï¼Œåªè¾“å‡ºè‹±æ–‡åŸå
    prompt = f"""
    Role: You are a Knowledge Graph Query Parser.
    Task: Convert the user's question into structured constraints.

    User Query: "{user_query}"

    Requirements:
    1. Identify atomic constraints.
    2. Property ID: Predict P-ID if sure (e.g. P57), else empty.
    3. Value: 
       - **DO NOT GUESS QIDs**. 
       - Output the exact **English Name** of the entity (e.g. "Chester Bennington", "Horror film").
       - For numbers/dates, keep them as is.
    4. Operator: =, >, <, contains.

    Output JSON List:
    [{{ "id": "c1", "property_id": "Pxx", "property_label": "...", "operator": "=", "value": "English Label Here", "softness": 0.0 }}]
    """

    try:
        data = llm.generate_json(prompt)
        constraints = []
        if isinstance(data, list):
            for item in data:
                label_value = str(item.get("value", ""))
                # === å®ä½“é“¾æ¥é€»è¾‘ ===
                # å¦‚æœä¸æ˜¯ QID ä¸”ä¸æ˜¯çº¯æ•°å­—/æ—¥æœŸï¼Œå°è¯•æœç´¢çœŸå® QID
                real_value = label_value
                if label_value and not re.match(r'^Q\d+$', label_value) and not re.match(r'^[\d\.\-\:]+$', label_value):
                    logger.info(f"Linking entity: '{label_value}' ...")
                    found_qid = search_wikidata(label_value)
                    if found_qid:
                        logger.info(f"  -> Found: {found_qid}")
                        real_value = found_qid
                    else:
                        logger.warning(f"  -> Not found, using original string.")

                c = Constraint(
                    id=item.get("id", "unknown"),
                    property_id=item.get("property_id", ""),
                    property_label=item.get("property_label", "unknown"),
                    operator=item.get("operator", "="),
                    value=real_value,
                    softness=float(item.get("softness", 0.0))
                )
                constraints.append(c)
        return constraints
    except Exception as e:
        logger.error(f"Parsing failed: {e}")
        return []


# ==============================================================================
# 3. æ ¸å¿ƒï¼šå›¾æ¨ç†æ‰§è¡Œå¼•æ“ (Graph Reasoning Engine)
# ==============================================================================
class GraphReasoningExecutor:
    """
    å®ç°â€œAnchor -> Step-by-Step Screeningâ€çš„æ‰§è¡Œé€»è¾‘ã€‚
    """

    def __init__(self, wikidata_service: WikidataService):
        self.service = wikidata_service
        self.trace = []  # ç”¨äºè®°å½•æ¨ç†è½¨è¿¹ (Evidence)

    def execute(self, sorted_constraints: List[Constraint]) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†å¹¶è¿”å›ç»“æœå’Œè¯æ®ã€‚
        Return: {
            "final_entities": [{"id": "Qxx", "label": "Saw 3D"}],
            "trace": ["Selected Anchor...", "Filtered by...", "Remaining..."]
        }
        """
        self.trace = []  # é‡ç½®è½¨è¿¹
        if not sorted_constraints:
            return {"final_entities": [], "trace": ["No constraints provided."]}

        # 1. Anchor é˜¶æ®µ
        anchor = sorted_constraints[0]
        anchor_log = f"Step 1 (Anchor): Started search with [{anchor.property_label} = {anchor.value}]."
        logger.info(anchor_log)
        self.trace.append(anchor_log)

        candidates = self._fetch_anchor_candidates(anchor)
        count_log = f"  -> Found {len(candidates)} initial candidates."
        logger.info(count_log)
        self.trace.append(count_log)

        if not candidates:
            return {"final_entities": [], "trace": self.trace}

        # 2. é€æ­¥ç­›é€‰ (Iterative Pruning)
        for i, constraint in enumerate(sorted_constraints[1:], 2):
            if not candidates:
                break

            step_log = f"Step {i} (Filter): Applying constraint [{constraint.property_label} {constraint.operator} {constraint.value}]."
            logger.info(step_log)
            self.trace.append(step_log)

            candidates = self._apply_filter(candidates, constraint)

            remain_log = f"  -> Candidates remaining: {len(candidates)}"
            logger.info(remain_log)
            self.trace.append(remain_log)

        # 3. è·å–æœ€ç»ˆç»“æœçš„è¯¦ç»†ä¿¡æ¯ (Label)
        final_details = self._fetch_labels_for_qids(candidates)

        return {
            "final_entities": final_details,
            "trace": self.trace
        }

    def _fetch_anchor_candidates(self, c: Constraint) -> Set[str]:
        """
        é’ˆå¯¹ Anchor èŠ‚ç‚¹ç”Ÿæˆåˆå§‹ SPARQL å¹¶æ‰§è¡Œã€‚
        """
        val_str = str(c.value)

        # æƒ…å†µ A: å·²ç»æ˜¯ QID (e.g., Q19198) - æœ€ç†æƒ³æƒ…å†µ
        if re.match(r'^Q\d+$', val_str):
            where_clause = f"?item wdt:{c.property_id} wd:{val_str} ."

        # æƒ…å†µ B: ä»ç„¶æ˜¯å­—ç¬¦ä¸² (Entity Linking å¤±è´¥)
        # æˆ‘ä»¬ä¸èƒ½ç›´æ¥æ¯”è¾ƒ ?item wdt:Pxx "String"ï¼Œå› ä¸ºå¯¹è±¡é€šå¸¸æ˜¯ URIã€‚
        # æˆ‘ä»¬éœ€è¦æŸ¥æ‰¾è¯¥å¯¹è±¡çš„ Label æ˜¯å¦åŒ¹é…å­—ç¬¦ä¸²ã€‚
        else:
            logger.info(f"Fallback: Searching by label match for {val_str} on property {c.property_id}")
            # è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒæ˜‚è´µçš„æ“ä½œï¼Œä½†æ¯”è¿”å›0ç»“æœè¦å¥½
            # é€»è¾‘ï¼š?item -> ?target_entity -> [Label == "Chester Bennington"]
            where_clause = f"""
                ?item wdt:{c.property_id} ?target .
                ?target rdfs:label ?targetLabel .
                FILTER(LCASE(STR(?targetLabel)) = LCASE("{val_str}")) .
                FILTER(LANG(?targetLabel) = "en") .
            """

        sparql = f"""
        SELECT DISTINCT ?item WHERE {{
            {where_clause}
        }}
        LIMIT 1000
        """

        # è°ƒè¯•ç”¨ï¼šæ‰“å°ç”Ÿæˆçš„ SPARQL
        print(f"DEBUG SPARQL:\n{sparql}")

        results = self.service.execute_sparql(sparql)

        qids = set()
        for r in results:
            url = r['item']['value']
            if "entity/" in url:
                qids.add(url.split("/")[-1])
        return qids

    def _apply_filter(self, current_candidates: Set[str], c: Constraint) -> Set[str]:
        """
        æ„é€  VALUES å­å¥ï¼Œå¯¹ç°æœ‰ candidates è¿›è¡Œ SPARQL è¿‡æ»¤ã€‚
        """
        # å°†å½“å‰å€™é€‰é›†è½¬æ¢ä¸º VALUES å­—ç¬¦ä¸² (e.g., "wd:Q1 wd:Q2 ...")
        # æ³¨æ„ï¼šå¦‚æœå€™é€‰é›†å¤ªå¤§ï¼Œå¯èƒ½éœ€è¦åˆ†æ‰¹å¤„ç†ã€‚è¿™é‡Œç®€åŒ–ä¸ºä¸€æ¬¡å¤„ç†ã€‚
        values_str = " ".join([f"wd:{qid}" for qid in current_candidates])

        val_str = str(c.value)
        is_qid = bool(re.match(r'^Q\d+$', val_str))
        is_date = bool(re.match(r'^\d{4}-\d{2}-\d{2}', val_str))
        is_number = val_str.replace('.', '', 1).isdigit()

        # æ„é€ è¿‡æ»¤é€»è¾‘
        filter_clause = ""
        target = f"wd:{val_str}" if is_qid else "?val"

        triple = f"?item wdt:{c.property_id} {target} ."

        if not is_qid:
            # æ„é€  FILTER è¡¨è¾¾å¼
            if is_date:
                val_fmt = f"'{val_str}'^^xsd:dateTime"
            elif is_number:
                val_fmt = val_str
            else:
                val_fmt = f"'{val_str}'"

            if c.operator == ">":
                filter_clause = f"FILTER(?val > {val_fmt})"
            elif c.operator == "<":
                filter_clause = f"FILTER(?val < {val_fmt})"
            elif c.operator == "contains":
                filter_clause = f"FILTER(CONTAINS(LCASE(?val), LCASE({val_fmt})))"
            else:
                filter_clause = f"FILTER(?val = {val_fmt})"

        sparql = f"""
        SELECT DISTINCT ?item WHERE {{
            VALUES ?item {{ {values_str} }}
            {triple}
            {filter_clause}
        }}
        """

        results = self.service.execute_sparql(sparql)

        # æå–ç¬¦åˆæ¡ä»¶çš„ QID
        valid_qids = set()
        for r in results:
            url = r['item']['value']
            valid_qids.add(url.split("/")[-1])

        return valid_qids

    def _fetch_labels_for_qids(self, qids: Set[str]) -> List[Dict[str, str]]:
        """
        æ ¹æ® QID è·å– Labelï¼Œä¸å†åªæ˜¯æ‰“å°ï¼Œè€Œæ˜¯è¿”å›æ•°æ®ç»“æ„
        """
        if not qids:
            return []

        # é™åˆ¶æ•°é‡ï¼Œé˜²æ­¢ Prompt è¿‡é•¿
        target_qids = list(qids)[:20]
        values_str = " ".join([f"wd:{qid}" for qid in target_qids])

        sparql = f"""
        SELECT ?item ?itemLabel WHERE {{
            VALUES ?item {{ {values_str} }}
            SERVICE wikibase:label {{ bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en". }}
        }}
        """
        results = self.service.execute_sparql(sparql)

        entities = []
        for r in results:
            url = r['item']['value']
            qid = url.split("/")[-1]
            label = r.get('itemLabel', {}).get('value', 'Unknown')
            entities.append({"id": qid, "label": label})

        return entities


def generate_final_response(user_query: str, execution_result: Dict, llm: LLMService):
    """
    æ¡†æ¶ç¬¬ 7 æ­¥ï¼šåŸºäºç­”æ¡ˆå’Œè¯æ®ç”Ÿæˆæœ€ç»ˆå›å¤ã€‚
    """
    logger.info("Phase 3: Generating Final Answer with LLM...")

    entities = execution_result["final_entities"]
    trace = execution_result["trace"]

    # 1. æ ¼å¼åŒ–è¯æ® (Evidence)
    trace_str = "\n".join(trace)

    # 2. æ ¼å¼åŒ–ç­”æ¡ˆ (Answers)
    if not entities:
        answer_str = "No specific entities were found matching all constraints."
    else:
        answer_str = ", ".join([f"{e['label']} ({e['id']})" for e in entities])

    # 3. æ„å»º Prompt
    prompt = f"""
    Role: You are an intelligent Knowledge Graph Question Answering Assistant.

    User Question: "{user_query}"

    System Execution Trace (Evidence of how the answer was found):
    {trace_str}

    Final Retrieved Entities from Knowledge Graph:
    {answer_str}

    Task: 
    Based ONLY on the provided evidence and retrieved entities, answer the user's question naturally. 
    1. Direct Answer: State the answer clearly.
    2. Explanation: Briefly explain the reasoning path (e.g., "We started by looking for... then filtered by...").
    3. If no results were found, explain which constraints might have been too strict based on the trace.
    """

    # 4. è°ƒç”¨ LLM
    final_response = llm.generate_text(prompt)

    print("\n" + "=" * 50)
    print("ğŸ¤– Final LLM Response:")
    print("=" * 50)
    print(final_response)
    print("=" * 50)

# ==============================================================================
# 4. ä¸»æµç¨‹
# ==============================================================================
def main():
    print("=== CCSP Framework: Graph of Thoughts Execution ===\n")

    # é…ç½® API (è¯·ä»ç¯å¢ƒå˜é‡æˆ–ç›´æ¥å¡«å…¥)
    api_key = os.getenv("LLM_API_KEY", "sk-wZPm2CCFydnh7Nuh9vuaMBLYiJxBxP0MsIMwp6rGZ87JVzkF")
    base_url = os.getenv("LLM_BASE_URL", "https://api.chatanywhere.tech/v1")
    model = "gpt-3.5-turbo"

    llm = LLMService(api_key, base_url,model)
    wiki_service = WikidataService()

    try:
        optimizer = ConstraintOptimizer("property_metadata_final.json", llm)
        logger.info("Optimizer loaded.")
    except Exception as e:
        logger.error(f"Init failed: {e}")
        return

    # ç¤ºä¾‹æŸ¥è¯¢
    user_query = "Which film starring Chester Bennington and directed by Kevin Greutert was released after 1995, is a horror film, and has a runtime shorter than 109.5 minutes?"
    print(f"Query: {user_query}\n")

    # 1. Parsing
    constraints = parse_query_to_constraints(user_query, llm)
    if not constraints: return

    # 2. Optimization (Planning)
    sorted_constraints = optimizer.optimize(constraints)

    print("\n--- Execution Plan ---")
    for i, c in enumerate(sorted_constraints):
        print(f"Step {i + 1}: {c.property_label} = {c.value} (Score: {c.priority_score:.2f})")

    # 3. Execution (Graph Reasoning)
    engine = GraphReasoningExecutor(wiki_service)

    # === ä¿®æ”¹ç‚¹ï¼šè·å–è¿”å›ç»“æœï¼Œè€Œä¸æ˜¯åªæ‰“å° ===
    execution_result = engine.execute(sorted_constraints)

    # 4. Final Generation (Step 7)
    # æŠŠæ‰€æœ‰ä¸Šä¸‹æ–‡é€ç»™ LLM åšæ€»ç»“
    generate_final_response(user_query, execution_result, llm)


if __name__ == "__main__":
    main()