import duckdb
import json
import time
import os
import requests
import math
from huggingface_hub import list_repo_files, hf_hub_download
from typing import List, Dict

# ================= é…ç½®åŒºåŸŸ =================
# 1. è®¾ç½®å›½å†…é•œåƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_TOKEN"] = "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"  # å»ºè®®æ­£å¼è¿è¡Œæ—¶é€šè¿‡ç¯å¢ƒå˜é‡è·å–

REPO_ID = "CleverThis/wikidata-truthy"
# æ³¨æ„ï¼šå…¨é‡æ¨¡å¼ä¸‹è¿™ä¸ªå˜é‡è™½ç„¶å®šä¹‰äº†ä½†å®é™…ä¸Šè¢«ä¸‹é¢çš„é€»è¾‘å¿½ç•¥äº†ï¼Œè¿™æ˜¯ç¬¦åˆé¢„æœŸçš„
SAMPLE_FILES_COUNT = 1600

# [ä¿®å¤] ä½¿ç”¨ raw string (r) é¿å… Windows è·¯å¾„è½¬ä¹‰é”™è¯¯
OUTPUT_FILE = r"/ccsp framework/property_metadata.json"


# ===========================================

def fetch_property_details(pids: List[str]) -> Dict[str, Dict]:
    """
    æ‰¹é‡è°ƒç”¨ Wikidata API è·å–å±æ€§çš„ Label å’Œ Descriptionã€‚
    å¢åŠ é‡è¯•æœºåˆ¶ï¼Œæé«˜å…¨é‡è·‘çš„ç¨³å®šæ€§ã€‚
    """
    print(f"   [API] æ­£åœ¨è·å– {len(pids)} ä¸ªå±æ€§çš„è¯­ä¹‰æè¿°...")

    url = "https://www.wikidata.org/w/api.php"
    results = {}

    headers = {"User-Agent": "CCSP-Research/1.0 (PropertyStatsBuilder)"}

    batch_size = 50
    for i in range(0, len(pids), batch_size):
        batch = pids[i: i + batch_size]
        ids_str = "|".join(batch)

        params = {
            "action": "wbgetentities",
            "ids": ids_str,
            "languages": "en",
            "props": "labels|descriptions",
            "format": "json"
        }

        # [ä¼˜åŒ–] å¢åŠ ç®€å•çš„é‡è¯•æœºåˆ¶
        retries = 3
        for attempt in range(retries):
            try:
                resp = requests.get(url, params=params, headers=headers, timeout=10)
                if resp.status_code == 429:  # é™æµ
                    time.sleep(5)
                    continue

                data = resp.json()

                if "entities" in data:
                    for pid, content in data["entities"].items():
                        label = content.get("labels", {}).get("en", {}).get("value", "Unknown")
                        desc = content.get("descriptions", {}).get("en", {}).get("value", "No description available.")
                        results[pid] = {"label": label, "description": desc}

                break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯

            except Exception as e:
                if attempt == retries - 1:
                    print(f"   [Error] APIè¯·æ±‚å¤±è´¥ (Batch {i}) ä¸”é‡è¯•è€—å°½: {e}")
                else:
                    time.sleep(2)  # ç­‰å¾…åé‡è¯•

        # ç¤¼è²Œæ€§å»¶æ—¶
        time.sleep(0.5)

        # ç®€å•è¿›åº¦æ˜¾ç¤º
        if (i + batch_size) % 1000 == 0:
            print(f"   [API è¿›åº¦] å·²å¤„ç† {i + batch_size}/{len(pids)}")

    return results


def run_pipeline():
    # --- ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æ•°æ® ---
    print(f"1. [ç¯å¢ƒ] æ£€æŸ¥ HuggingFace ç¼“å­˜ (å…¨é‡æ¨¡å¼: {REPO_ID})...")

    try:
        all_files = list_repo_files(repo_id=REPO_ID, repo_type="dataset")
        target_files = [f for f in all_files if f.endswith(".parquet")]
        target_files.sort()

        print(f"   [å…¨é‡å‡†å¤‡] å…±å‘ç° {len(target_files)} ä¸ª Parquet æ–‡ä»¶ï¼Œå‡†å¤‡åŠ è½½...")

        local_paths = []
        for idx, filename in enumerate(target_files):
            path = hf_hub_download(repo_id=REPO_ID, filename=filename, repo_type="dataset")
            local_paths.append(path)

            if (idx + 1) % 50 == 0 or (idx + 1) == len(target_files):
                print(f"   è¿›åº¦: {idx + 1}/{len(target_files)} æ–‡ä»¶å·²å°±ç»ª")

    except Exception as e:
        print(f"   [Fatal] æ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨æˆ–ä¸‹è½½å¤±è´¥: {e}")
        return

    # --- ç¬¬äºŒæ­¥ï¼šDuckDB ç»Ÿè®¡ ---
    print("2. [è®¡ç®—] DuckDB å…¨é‡èšåˆ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿ)...")
    start_time = time.time()

    # [ä¼˜åŒ–] ä½¿ç”¨ç£ç›˜æ”¯æŒçš„æ•°æ®åº“ï¼Œé¿å…å†…å­˜æº¢å‡º (OOM)
    # å¤„ç†å®Œåå¯ä»¥æ‰‹åŠ¨åˆ é™¤ temp_stats.duckdb
    con = duckdb.connect("temp_stats.duckdb")

    # å¢åŠ å†…å­˜é™åˆ¶ (æ ¹æ®ä½ çš„æœºå™¨è°ƒæ•´ï¼Œä¾‹å¦‚ '16GB')
    # con.execute("PRAGMA memory_limit='16GB'")
    # con.execute("PRAGMA threads=8") # åˆ©ç”¨å¤šæ ¸

    # æ³¨æ„ï¼šlocal_paths å¦‚æœæ–‡ä»¶å¤ªå¤šï¼ŒSQLå­—ç¬¦ä¸²å¯èƒ½è¿‡é•¿ã€‚
    # DuckDBæ”¯æŒç›´æ¥ä¼ åˆ—è¡¨ï¼Œä½†åœ¨SQLä¸­éœ€è¦æ ¼å¼åŒ–å¥½ã€‚
    # è¿™é‡Œä¿æŒä½ çš„é€»è¾‘ï¼Œå› ä¸ºé€šå¸¸å‡ åƒä¸ªæ–‡ä»¶çš„è·¯å¾„å­—ç¬¦ä¸²è¿˜æ˜¯åœ¨é™åˆ¶å†…çš„ã€‚

    query = fr"""
    SELECT 
        regexp_extract(predicate, 'P\d+', 0) as pid,
        COUNT(*) as cnt,
        APPROX_COUNT_DISTINCT("object") as unique_cnt
    FROM read_parquet({local_paths})
    WHERE regexp_matches(predicate, 'P\d+')
    GROUP BY pid
    HAVING cnt > 10
    ORDER BY cnt DESC
    """

    print("   [DuckDB] å¼€å§‹æ‰§è¡Œ SQL (Aggregation)...")
    df = con.execute(query).df()

    total_sampled_rows = df['cnt'].sum()
    print(f"   ç»Ÿè®¡å®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}s")
    print(f"   åˆ†æä¸‰å…ƒç»„æ€»æ•°: {total_sampled_rows:,}")
    print(f"   å‘ç°æœ‰æ•ˆå±æ€§: {len(df)} ä¸ª")

    # å…³é—­è¿æ¥ï¼Œé‡Šæ”¾é”
    con.close()

    # å¯é€‰ï¼šåˆ é™¤ä¸´æ—¶æ•°æ®åº“æ–‡ä»¶
    if os.path.exists("temp_stats.duckdb"):
        try:
            os.remove("temp_stats.duckdb")
            print("   [System] ä¸´æ—¶æ•°æ®åº“å·²æ¸…ç†")
        except:
            pass

    # --- ç¬¬ä¸‰æ­¥ï¼šè·å–è¯­ä¹‰æè¿° (API) ---
    all_pids = df['pid'].tolist()
    semantic_data = fetch_property_details(all_pids)

    # --- ç¬¬å››æ­¥ï¼šæ„å»ºæœ€ç»ˆå…ƒæ•°æ® ---
    print("4. [åˆå¹¶] ç”Ÿæˆæœ€ç»ˆ JSON æ–‡ä»¶...")

    metadata = {
        "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "source": "Wikidata-Truthy Full Dump",
        "total_triples_analyzed": int(total_sampled_rows),
        "file_count": len(target_files),
        "properties": {}
    }

    for _, row in df.iterrows():
        pid = row['pid']
        cnt = int(row['cnt'])
        unique_raw = int(row['unique_cnt'])
        unique = min(unique_raw, cnt)
        cr_val = unique / cnt if cnt > 0 else 0.0
        semantics = semantic_data.get(pid, {"label": "Unknown", "description": "No description."})

        metadata["properties"][pid] = {
            "label": semantics["label"],
            "description": semantics["description"],
            "cnt": cnt,
            "cr": round(cr_val, 6),
            "stats": {
                "total": cnt,
                "unique": unique
            }
        }

    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out:
        json.dump(metadata, out, indent=2, ensure_ascii=False)

    print(f"ğŸ‰ å…¨é‡ç»Ÿè®¡æˆåŠŸ! æ–‡ä»¶å·²ä¿å­˜è‡³: {OUTPUT_FILE}")


if __name__ == "__main__":
    run_pipeline()