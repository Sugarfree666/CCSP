import json
import re
import time
from SPARQLWrapper import SPARQLWrapper, JSON

# ================= é…ç½®åŒºåŸŸ =================
# === é…ç½® ===
INPUT_FILE = r"D:\GitHub\CCSP\datasets\complex_constraint_dataset_rewrite_queries.json"  # ä½ çš„é—®é¢˜é›†
METADATA_FILE = r"D:\GitHub\CCSP\ccsp framework\property_metadata.json"  # ä½ çš„ç»Ÿè®¡è¡¨
OUTPUT_FILE = "train_data_pointwise.jsonl"

# åˆ¤å®šæ ‡å‡†ï¼šç»“æœæ•°é‡åœ¨ [1, 1000] ä¹‹é—´ä¸ºå¥½é”šç‚¹
MIN_ANCHOR_SIZE = 1
MAX_ANCHOR_SIZE = 1000

# SPARQL ç«¯ç‚¹
SPARQL_ENDPOINT = "https://query.wikidata.org/sparql"


# ===========================================

class DatasetBuilderFinal:
    def __init__(self):
        self.metadata = self._load_json(METADATA_FILE).get("properties", {})
        self.sparql = SPARQLWrapper(SPARQL_ENDPOINT)
        self.sparql.setReturnFormat(JSON)
        self.sparql.addCustomHttpHeader("User-Agent", "CCSP-DatasetBuilder/3.1 (Research)")

    def _load_json(self, path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[Error] Failed to load {path}: {e}")
            return {}

    def get_stats_text(self, pid):
        """ç”Ÿæˆç»Ÿè®¡ç‰¹å¾æ–‡æœ¬ (Feature Injection)"""
        meta = self.metadata.get(pid, {})
        if not meta:
            return "Frequency: Unknown, Diversity: Unknown"

        cnt = meta.get("cnt", 0)
        cr = meta.get("cr", 0.0)

        if cnt > 1000000:
            freq = "Universal (>1M)"
        elif cnt > 10000:
            freq = "Common"
        elif cnt > 100:
            freq = "Moderate"
        else:
            freq = "Rare"

        if cr > 0.9:
            div = "Unique Identifier"
        elif cr > 0.1:
            div = "High Diversity"
        else:
            div = "Low Diversity"

        return f"Frequency: {freq}, Diversity: {div} (CR:{cr:.2f})"

    def get_real_count_limit(self, query_sparql):
        """[æ ¸å¿ƒä¼˜åŒ–] ä½¿ç”¨ LIMIT æ£€æµ‹æ³•è·å–æ•°é‡"""
        try:
            limit_val = MAX_ANCHOR_SIZE + 1
            if "LIMIT" not in query_sparql.upper():
                query_sparql += f" LIMIT {limit_val}"

            self.sparql.setQuery(query_sparql)
            # å»ºè®®ç¨å¾®æ”¾å®½ä¸€ç‚¹è¶…æ—¶æ—¶é—´ï¼Œæˆ–è€…ä¿æŒ 10s
            self.sparql.setTimeout(15)

            results = self.sparql.query().convert()["results"]["bindings"]
            count = len(results)

            if count >= limit_val:
                return 999999  # æº¢å‡ºï¼ŒBad Anchor
            return count

        except Exception as e:
            error_str = str(e).lower()  # è½¬å°å†™ï¼Œé€šæ€æ‰€æœ‰å¤§å°å†™æƒ…å†µ

            # æ•è·å„ç§è¶…æ—¶æƒ…å†µ
            if "timed out" in error_str or "timeout" in error_str or "504" in error_str:
                # æ ¸å¿ƒä¿®æ”¹ï¼šè¶…æ—¶ = ææ…¢ = Bad Anchor
                # ä¸è¦æ‰“å°é”™è¯¯åˆ·å±ï¼Œç›´æ¥è¿”å›å¤§æ•°
                return 999999

                # å…¶ä»–é”™è¯¯æ‰æ‰“å°
            print(f"   [SPARQL Error]: {e}")
            return -1

    def recover_subject_anchor(self, simple_question_text, answer_qid):
        """
        [ç­–ç•¥ä¼˜åŒ–] ä½¿ç”¨ åŸå§‹ç®€å•é—®é¢˜ (simple_question_text) è¿›è¡Œå®ä½“åŒ¹é…
        ç†ç”±ï¼šåŸå§‹é—®é¢˜ä¸­çš„å®ä½“é€šå¸¸æœªç»å˜å½¢ï¼ŒåŒ¹é…æˆåŠŸç‡æ›´é«˜
        """
        query = f"""
        SELECT ?neighbor ?neighborLabel ?p ?dir WHERE {{
          {{ ?neighbor ?p wd:{answer_qid} . BIND("incoming" AS ?dir) }} 
          UNION 
          {{ wd:{answer_qid} ?p ?neighbor . BIND("outgoing" AS ?dir) }}
          SERVICE wikibase:label {{ bd:serviceParam wikibase:language "en". }}
        }} LIMIT 200
        """

        candidates = []
        try:
            self.sparql.setQuery(query)
            results = self.sparql.query().convert()["results"]["bindings"]

            # ä½¿ç”¨ç®€å•é—®é¢˜è¿›è¡ŒåŒ¹é…
            q_lower = simple_question_text.lower()

            for row in results:
                lbl = row.get("neighborLabel", {}).get("value", "").strip()
                pid = row["p"]["value"].split("/")[-1]

                if pid in ["P31", "P17", "P279", "P131", "P_score"]: continue
                if not lbl: continue

                # === æ ¸å¿ƒåŒ¹é…ï¼šåœ¨ç®€å•é—®é¢˜ä¸­å¯»æ‰¾ ===
                if len(lbl) > 2 and lbl.lower() in q_lower:
                    neighbor_qid = row["neighbor"]["value"].split("/")[-1]
                    direction = row["dir"]["value"]

                    candidates.append({
                        "type": "recovered",
                        "pid": pid,
                        "subject_label": lbl,
                        "subject_qid": neighbor_qid,
                        "direction": direction
                    })
        except:
            pass

        return candidates

    def parse_filter_constraints(self, logic_str):
        if not logic_str: return []
        constraints = []
        parts = logic_str.split(" AND ")
        for part in parts:
            clean_part = part.strip("() ")
            match = re.match(r"(P\d+)\s+(is|[<>=]+)\s+(.+)", clean_part)
            if match:
                pid, op_str, val_raw = match.groups()
                op = "=" if op_str == "is" else op_str
                val = val_raw.strip("'\"")
                constraints.append({
                    "type": "filter",
                    "pid": pid,
                    "op": op,
                    "val": val
                })
        return constraints

    def process(self):
        questions = self._load_json(INPUT_FILE)
        print(f"ğŸš€ Processing {len(questions)} questions...")

        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            for idx, item in enumerate(questions):
                # å…³é”®ä¿®æ”¹ï¼šåˆ†ç¦»ç”¨é€”
                complex_q_text = item['complex_question']  # ç”¨ä½œè®­ç»ƒç‰¹å¾ (Feature)
                simple_q_text = item['original_question']  # ç”¨ä½œæŒ–æ˜åŒ¹é… (Mining)

                # === 1. å°è¯•æ‰¾å›æ­£æ ·æœ¬ (Label 1) ===
                if item.get("new_ground_truth"):
                    ans_qid = item["new_ground_truth"][0]

                    # ä½¿ç”¨ ç®€å•é—®é¢˜ æ‰¾å› Anchor
                    recovered_anchors = self.recover_subject_anchor(simple_q_text, ans_qid)

                    for anchor in recovered_anchors:
                        # éªŒè¯ Count
                        if anchor['direction'] == "incoming":
                            sparql = f"SELECT ?s WHERE {{ wd:{anchor['subject_qid']} wdt:{anchor['pid']} ?s }}"
                        else:
                            sparql = f"SELECT ?s WHERE {{ ?s wdt:{anchor['pid']} wd:{anchor['subject_qid']} }}"

                        count = self.get_real_count_limit(sparql)

                        # ç”Ÿæˆæ–‡æœ¬
                        stats = self.get_stats_text(anchor['pid'])
                        pid_label = self.metadata.get(anchor['pid'], {}).get('label', anchor['pid'])

                        cand_text = f"Constraint: {pid_label} ({anchor['pid']}) = '{anchor['subject_label']}'. Stats: {stats}"

                        label = 1.0 if MIN_ANCHOR_SIZE <= count <= MAX_ANCHOR_SIZE else 0.0

                        # å†™å…¥è®­ç»ƒæ•°æ®æ—¶ï¼ŒQuery ä½¿ç”¨ å¤æ‚é—®é¢˜
                        record = {"query": complex_q_text, "text": cand_text, "label": label}
                        f.write(json.dumps(record, ensure_ascii=False) + "\n")
                        print(f"   [Recovered] {cand_text[:60]}... -> Count:{count} (Label {label})")

                # === 2. å¤„ç†åŸæœ‰ Filters (é€šå¸¸æ˜¯ Label 0) ===
                filters = self.parse_filter_constraints(item.get('constraint_logic', ''))
                for filt in filters:
                    if filt['op'] in ['>', '<', '>=', '<=']:
                        count = 999999
                    else:
                        safe_val = filt['val'].replace("'", "\\'")
                        sparql = f"""
                        SELECT ?s WHERE {{ 
                            ?s wdt:{filt['pid']} ?o .
                            ?o rdfs:label ?label .
                            FILTER(LCASE(STR(?label)) = LCASE("{safe_val}")) .
                            FILTER(LANG(?label) = "en")
                        }}
                        """
                        count = self.get_real_count_limit(sparql)

                    stats = self.get_stats_text(filt['pid'])
                    pid_label = self.metadata.get(filt['pid'], {}).get('label', filt['pid'])
                    cand_text = f"Constraint: {pid_label} ({filt['pid']}) {filt['op']} '{filt['val']}'. Stats: {stats}"

                    label = 1.0 if MIN_ANCHOR_SIZE <= count <= MAX_ANCHOR_SIZE else 0.0

                    # å†™å…¥è®­ç»ƒæ•°æ®æ—¶ï¼ŒQuery ä¾ç„¶ä½¿ç”¨ å¤æ‚é—®é¢˜
                    record = {"query": complex_q_text, "text": cand_text, "label": label}
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")

                    if label == 1.0:
                        print(f"   [Filter]    {cand_text[:60]}... -> Count:{count} (Label {label})")

                time.sleep(0.5)

        print(f"Done! Saved to {OUTPUT_FILE}")


if __name__ == "__main__":
    builder = DatasetBuilderFinal()
    builder.process()