import duckdb
import json
import time
import os
import math
from huggingface_hub import list_repo_files, hf_hub_download

# ================= é…ç½®åŒºåŸŸ =================
# 1. è®¾ç½®å›½å†…é•œåƒ (ç¡®ä¿ä¸‹è½½é€Ÿåº¦)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# è¯·ç¡®ä¿ Token æœ‰æ•ˆï¼Œæˆ–è€…å¦‚æœåœ¨æœ¬åœ°å·²ç™»å½•å¯æ³¨é‡Šæ‰ä¸‹è¡Œ
os.environ["HF_TOKEN"] ="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"

REPO_ID = "CleverThis/wikidata-truthy"
SAMPLE_FILES_COUNT = 100  # å»ºè®® 50-100 ä¸ªæ–‡ä»¶ä»¥è¦†ç›–é•¿å°¾å±æ€§
OUTPUT_FILE = "ccsp framework/property_metadata_final.json"
# ===========================================

def run_pipeline():
    # --- ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æ•°æ® ---
    print(f"1. [ç½‘ç»œ] è¿æ¥é•œåƒç«™: {os.environ.get('HF_ENDPOINT')} ...")

    try:
        all_files = list_repo_files(repo_id=REPO_ID, repo_type="dataset")
        parquet_files = [f for f in all_files if f.endswith(".parquet")]
        parquet_files.sort()
        target_files = parquet_files[:SAMPLE_FILES_COUNT]
        print(f"   é€‰ä¸­æ–‡ä»¶æ•°: {len(target_files)}")
    except Exception as e:
        print(f"   é”™è¯¯: æ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨ ({e})")
        return

    print(f"2. [ä¸‹è½½] ç¼“å­˜ {SAMPLE_FILES_COUNT} ä¸ª Parquet æ–‡ä»¶...")
    local_paths = []
    for idx, filename in enumerate(target_files):
        path = hf_hub_download(repo_id=REPO_ID, filename=filename, repo_type="dataset")
        local_paths.append(path)
        if (idx + 1) % 10 == 0: print(f"   è¿›åº¦: {idx + 1}/{SAMPLE_FILES_COUNT}")

    # --- ç¬¬äºŒæ­¥ï¼šDuckDB ç»Ÿè®¡ ---
    print("3. [è®¡ç®—] DuckDB èšåˆ (ç»Ÿè®¡ Total å’Œ Unique)...")
    start_time = time.time()

    con = duckdb.connect()
    # SQL: æå– Pxxx, ç»Ÿè®¡æ€»æ•°, ç»Ÿè®¡å»é‡æ•°
    query = f"""
    SELECT 
        regexp_extract(predicate, 'P\d+', 0) as pid,
        COUNT(*) as total_count,
        APPROX_COUNT_DISTINCT("object") as unique_count
    FROM read_parquet({local_paths})
    GROUP BY pid
    ORDER BY total_count DESC
    """
    df = con.execute(query).df()

    total_sampled_rows = df['total_count'].sum()
    print(f"   ç»Ÿè®¡å®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}s")
    print(f"   æ€»ä¸‰å…ƒç»„è¡Œæ•°: {total_sampled_rows:,}")

    # --- ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æŒ‡æ ‡å¹¶ä¿å­˜ ---
    print("4. [ç”Ÿæˆ] è®¡ç®— s_base, lambda, CR å¹¶ä¿å­˜...")

    metadata = {
        "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "source": "DuckDB Sampling with Log-Cardinality",
        "total_rows_analyzed": int(total_sampled_rows),
        "properties": {}
    }

    # è·å– P31 è®¡æ•° (ç”¨äºè®¡ç®— r çš„åˆ†æ¯)
    p31_row = df[df['pid'] == 'P31']
    p31_count = int(p31_row['total_count'].iloc[0]) if not p31_row.empty else total_sampled_rows

    for _, row in df.iterrows():
        pid = row['pid']
        count = int(row['total_count'])
        unique = int(row['unique_count'])

        if count < 10: continue

        # 1. è®¡ç®— r(v) (Reliability)
        r_val = min(count / p31_count, 1.0)

        # 2. è®¡ç®— CR (Linear Cardinality Ratio)
        # CR = Unique / Total
        cr_val = unique / count

        # 3. è®¡ç®— s_base (New Log Formula)
        # å…¬å¼: 0.2 + 0.8 * (ln(U+1) / ln(T+1))
        if count <= 1:
            s_base = 0.2
        else:
            log_unique = math.log(unique + 1)
            log_total = math.log(count + 1)
            ratio = log_unique / log_total
            s_base = 0.2 + (0.8 * ratio)
        s_base = min(s_base, 1.0)

        # 4. è®¡ç®— lambda (LLM Weight)
        # å…¬å¼: 0.8 * (1 - CR)
        # CR è¶Šé«˜(IDç±»)ï¼Œlambda è¶Šä½(ä¸ä¿¡LLM)
        lambda_val = 0.8 * (1.0 - cr_val)
        lambda_val = max(0.0, lambda_val)  # ä¿è¯éè´Ÿ

        metadata["properties"][pid] = {
            # "label": label_text,  # å·²ç§»é™¤
            "r": round(r_val, 6),  # å¯†åº¦
            "s_base": round(s_base, 6),  # åŸºç¡€åŒºåˆ†åº¦ (å¯¹æ•°ç‰ˆ)
            "lambda": round(lambda_val, 6),  # LLM æƒé‡ (çº¿æ€§ç‰ˆ)
            "CR": round(cr_val, 6),  # åŸå§‹ CR ç”¨äºåˆ†æ
            "stats": {  # è®°å½•åŸå§‹ç»Ÿè®¡æ•°æ®æ–¹ä¾¿Debug
                "total": count,
                "unique": unique
            }
        }

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out:
        json.dump(metadata, out, indent=2, ensure_ascii=False)

    print(f"ğŸ‰ æˆåŠŸ! å…ƒæ•°æ®è¡¨å·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    print(f"   å…±æ”¶å½•å±æ€§: {len(metadata['properties'])} ä¸ª")


if __name__ == "__main__":
    run_pipeline()